{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from ConceptBottleneck.CUB.models import *\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test Picture of a Dog and related transformation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torchvision import transforms\n",
    "input_image = Image.open('docs/dog.jpg')\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Default Model From pytorch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\cheng/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5696e-01,  1.1051e-01, -4.4363e-01, -6.1477e-02, -2.0094e-01,\n",
      "         1.6820e-01,  5.8117e-01,  6.0334e-02, -7.7651e-02, -1.1192e+00,\n",
      "        -2.1800e-01, -5.2624e-01, -1.1816e-01,  6.6277e-02,  8.4988e-01,\n",
      "        -5.3259e-02, -4.7113e-01, -2.5980e-03,  2.7504e-01,  1.7060e-01,\n",
      "         4.4231e-01, -3.8615e-01,  1.3482e-01, -5.2201e-01, -3.3516e-04,\n",
      "        -2.3492e-01, -2.3264e-01, -7.1883e-03,  1.6899e-02, -1.2724e-01,\n",
      "         2.0508e-01, -6.9888e-03,  1.3074e+00, -2.3181e-01,  9.8966e-01,\n",
      "        -7.7782e-01,  4.8900e-01, -4.2243e-01, -2.6523e-01, -1.5005e-01,\n",
      "         3.5147e-01, -1.7462e-01,  1.0251e-01,  4.2223e-01, -1.4342e-01,\n",
      "         1.3578e-02, -4.6477e-01,  3.5966e-01,  1.5194e-01,  2.0217e-01,\n",
      "        -1.3763e-01,  1.4986e-01,  2.8415e-01, -3.7086e-01,  9.7689e-01,\n",
      "         7.9543e-01, -7.4032e-01, -7.8852e-02, -9.6364e-02,  1.2193e-01,\n",
      "        -1.2712e-01,  3.6825e-01, -4.5354e-01, -7.4801e-01, -7.9354e-02,\n",
      "         2.1688e-01,  2.7018e-01,  1.0089e+00,  4.0730e-01, -1.8570e-01,\n",
      "        -9.2470e-02, -1.2458e-01, -1.0435e+00, -4.0775e-01,  1.8339e-01,\n",
      "        -2.7525e-01, -2.9294e-01,  1.6332e-02,  2.8761e-02,  2.0185e-01,\n",
      "        -4.8364e-01, -7.0668e-02,  1.6896e-01, -4.6968e-02,  8.4163e-01,\n",
      "        -3.6728e-01, -3.0113e-01, -3.7112e-01,  2.6911e-01,  3.7276e-01,\n",
      "         1.8055e-01,  3.1312e-01,  4.5380e-01, -1.3350e-01, -2.9443e-01,\n",
      "        -2.7947e-01,  8.4207e-01, -2.3105e-01, -3.1554e-01, -4.3883e-02,\n",
      "         1.5329e-01,  3.1552e-01,  1.8616e-01, -9.6860e-02,  4.7281e-01,\n",
      "        -8.9032e-01, -7.2464e-01,  2.3588e-01,  6.6725e-01,  1.3802e-01,\n",
      "        -2.4249e-01, -2.3605e-02, -5.4462e-02,  4.9207e-01,  4.9702e-02,\n",
      "        -5.8998e-01, -7.7230e-01,  1.3981e-01, -8.2220e-01,  3.9936e-02,\n",
      "         1.7365e-01, -1.0092e+00, -6.9959e-03, -2.6625e-01, -2.8960e-01,\n",
      "        -6.6461e-02, -6.4817e-01,  9.3626e-01,  5.5003e-01,  5.2842e-01,\n",
      "         1.0856e-01,  1.8085e-01,  6.0892e-01, -6.0835e-01,  3.3635e-01,\n",
      "         1.8864e-01,  5.8985e-01, -4.0940e-01, -7.3610e-01, -6.2800e-01,\n",
      "        -2.7593e-01,  2.6514e-01, -7.5665e-01,  2.5088e-01,  9.6007e-01,\n",
      "        -9.4859e-02,  3.0636e-01, -2.7369e-02,  3.5944e-02,  1.7601e-01,\n",
      "        -2.8860e-01,  1.6987e-01,  2.4232e+00, -1.2510e-01,  9.1045e-01,\n",
      "        -8.8260e-01, -5.1855e-02,  2.1003e+00, -6.6269e-01, -2.4619e-01,\n",
      "        -8.5638e-01, -1.3153e+00, -1.1042e+00, -2.4547e-01, -1.3737e-01,\n",
      "        -1.1962e-01, -2.0865e-01, -3.6327e-01, -9.5907e-02,  7.0066e-01,\n",
      "        -2.2363e-01, -1.8417e-01, -8.5439e-01, -1.8438e-01,  1.3066e+00,\n",
      "        -4.8575e-01, -5.6697e-01, -1.9955e-01, -1.6855e-01, -1.4573e-01,\n",
      "        -8.5888e-01, -3.4339e-01, -2.9275e-01, -4.5627e-01, -5.1301e-01,\n",
      "         4.3903e-01,  8.0301e-01, -4.9003e-01, -6.9808e-01, -8.5991e-01,\n",
      "        -4.1795e-01, -4.4660e-01,  2.0681e-01, -1.2265e+00, -4.5363e-01,\n",
      "        -5.2789e-01, -1.1172e+00,  2.6781e-01, -4.5858e-01,  2.8015e-01,\n",
      "         6.0328e-01, -8.8579e-01, -8.8179e-01,  1.4860e+00, -4.7116e-01,\n",
      "        -1.1044e+00, -5.9475e-01,  6.3241e-01, -5.9048e-01, -7.0500e-01,\n",
      "        -3.8798e-01, -6.3180e-01, -2.1158e-01, -1.6535e-01,  6.3881e-01,\n",
      "         2.8981e-02,  4.5989e-01,  8.4380e-02, -5.4092e-02, -1.9021e+00,\n",
      "        -1.1602e-01, -2.3897e-01,  2.5432e+00,  1.7504e+00,  1.3806e+00,\n",
      "        -5.3402e-01,  3.4183e-01, -7.0692e-01,  3.1853e-01, -9.9139e-02,\n",
      "         1.1112e+00,  1.2860e+00,  1.0465e+00, -7.3276e-02, -2.3343e-01,\n",
      "         6.5050e-02,  5.4067e-01, -4.5558e-01, -8.5781e-02, -4.2102e-01,\n",
      "         7.2590e-01,  4.5586e-01, -8.7704e-02, -8.4019e-01, -3.2783e-01,\n",
      "        -5.2903e-01, -4.2849e-01,  4.9993e-01,  3.2688e+00,  2.3975e+00,\n",
      "         1.5311e+00, -2.0808e-01, -1.8428e-01, -2.8011e-01, -1.7841e-01,\n",
      "         2.8461e-01,  5.3146e-01,  2.8738e+00,  8.8523e+00,  4.2019e+00,\n",
      "         2.5316e+00,  3.9927e+00, -3.1314e-01, -6.2400e-01, -2.6323e-01,\n",
      "        -2.3428e-01, -3.3132e-01,  1.7870e-01, -4.0714e-01,  8.2185e-01,\n",
      "         4.6482e+00, -8.9972e-01, -3.8765e-01,  6.0829e-01,  3.4514e-01,\n",
      "        -8.4613e-01, -7.5298e-02,  5.9165e-01, -2.2571e-01,  4.7909e+00,\n",
      "         2.8114e-01, -3.3872e-01, -7.5726e-01,  1.2868e+00, -6.3885e-01,\n",
      "        -7.2032e-01, -1.0338e+00,  2.1799e-01, -5.7311e-02,  1.2854e-01,\n",
      "        -4.6507e-01,  3.5067e-02, -3.6578e-01, -4.7804e-02, -9.9203e-01,\n",
      "        -1.3356e+00,  6.5033e-01, -2.7167e-01, -6.8011e-01, -5.7143e-01,\n",
      "        -5.3475e-01, -3.8411e-01, -6.4809e-02, -2.8354e-01, -1.7806e-01,\n",
      "        -2.8929e-02,  2.4240e-01, -1.7973e-01,  1.8139e-01, -1.5519e-01,\n",
      "        -2.3687e-01,  1.5365e-01, -1.1674e-01, -2.9109e-01,  2.4691e-01,\n",
      "         2.1827e-01, -5.1025e-01,  2.5870e-01, -1.6245e-01, -2.7104e-01,\n",
      "         4.1199e-01, -3.9074e-01, -6.5319e-01,  2.0439e-01, -1.2711e+00,\n",
      "        -4.3313e-01,  1.5427e-01,  3.4154e-01,  8.7113e-01, -4.1167e-02,\n",
      "        -1.8232e-01,  3.9799e-01,  9.8626e-01, -4.9940e-01, -2.1770e-02,\n",
      "        -5.8458e-01, -9.3022e-01, -5.0841e-01,  1.4684e-02, -2.6315e-01,\n",
      "         1.1865e-02, -1.7343e-01, -2.2880e-01, -2.0762e-01, -2.8182e-01,\n",
      "         3.0906e-02,  5.7570e-01,  3.2597e-01, -1.9836e-01, -8.1336e-01,\n",
      "        -5.2461e-01, -1.6578e-01, -4.2071e-01, -1.4268e-01, -2.6060e-01,\n",
      "         1.3963e-01, -8.4470e-02, -2.1359e-01,  7.6789e-01,  5.5213e-01,\n",
      "        -1.0924e+00,  6.3716e-01, -7.3622e-01,  4.3754e-01, -7.7834e-01,\n",
      "        -7.9443e-01, -1.4501e+00, -7.9228e-01, -7.8784e-01, -8.8925e-01,\n",
      "         2.0367e-01, -4.4672e-01, -9.1861e-01,  6.8159e-02, -2.8454e-01,\n",
      "        -3.7750e-01, -9.7066e-01,  1.0829e-01, -4.9947e-01, -1.3005e+00,\n",
      "        -7.6300e-01, -6.5802e-01, -7.3246e-01, -1.0341e+00, -1.2842e+00,\n",
      "        -3.6779e-01, -9.8348e-01, -9.7599e-02, -4.1650e-01, -6.8750e-02,\n",
      "        -3.5764e-01, -3.6454e-01,  2.1046e-01,  5.5320e-01, -2.1323e-01,\n",
      "        -2.6107e-01,  4.5051e-01, -7.4766e-02,  5.1910e-01,  1.7004e-01,\n",
      "        -3.2709e-02, -1.0269e+00,  5.5582e-01,  1.5760e-01,  9.7135e-02,\n",
      "         1.8764e-01, -3.2566e-01, -3.2677e-01,  2.5147e-01, -2.8530e-01,\n",
      "        -5.9853e-01, -1.0255e+00, -5.6512e-01, -3.8385e-01, -4.1670e-01,\n",
      "        -5.8828e-01, -6.2122e-01,  1.9794e-01, -4.6862e-01,  1.3163e-01,\n",
      "        -2.0325e-01, -1.0149e+00, -2.3162e-01,  1.7283e-01, -4.8196e-01,\n",
      "         2.5326e-01,  1.6461e-01,  6.2808e-02, -2.2099e-01,  2.0254e-01,\n",
      "        -1.6538e-01, -3.9706e-01,  2.4437e-01,  1.6360e-01, -7.2684e-01,\n",
      "        -1.0392e-02, -1.9012e-02, -3.1348e-01, -5.3056e-01,  8.2013e-02,\n",
      "         5.7136e-01, -3.0761e-01,  3.8100e-01, -6.4842e-01,  8.9464e-02,\n",
      "         4.7818e-01, -7.8970e-01, -3.4117e-01,  4.7725e-01,  1.5669e-01,\n",
      "        -1.8339e-01, -1.3243e-01,  2.3159e-01, -5.0399e-01, -2.2604e-01,\n",
      "         7.9325e-02,  5.2133e-01,  1.5784e-01, -8.6526e-02,  3.0305e-01,\n",
      "         9.7812e-02, -1.2771e+00,  7.0310e-01,  1.4597e-01,  3.2879e-01,\n",
      "        -3.6155e-01, -6.2039e-01, -8.8755e-01,  4.2935e-01,  1.1274e-01,\n",
      "         3.2891e-01,  8.4745e-02,  6.9848e-02,  8.1302e-01, -8.0782e-01,\n",
      "        -6.8245e-01,  1.3358e+00, -2.6256e-01,  8.9051e-01,  1.6758e-01,\n",
      "        -1.2170e-01,  4.2807e-01, -1.0054e+00,  3.5863e-01,  3.6241e-01,\n",
      "        -8.2987e-01,  2.9833e-01,  3.9783e-02,  3.5029e-01, -1.9861e-04,\n",
      "         1.4194e-02,  4.7898e-01, -5.8930e-01, -2.8985e-01, -1.0249e-01,\n",
      "         7.4155e-01,  2.6986e-01, -4.4872e-01, -1.5987e-01,  4.8277e-01,\n",
      "        -6.3190e-01, -2.1434e-01,  5.0953e-01, -6.9076e-01, -7.4333e-01,\n",
      "         2.0042e-01,  7.0911e-02,  1.0657e-02,  1.4495e-01, -6.7097e-01,\n",
      "         2.4766e-01,  7.7923e-01,  5.2100e-02, -3.9851e-01,  1.6331e-01,\n",
      "         1.0057e-01, -9.3775e-01, -5.3325e-01, -7.2511e-01, -1.2609e+00,\n",
      "        -2.4719e-01,  7.4883e-01,  1.0540e+00, -6.8609e-01, -1.6086e-01,\n",
      "        -4.3294e-01, -5.9875e-01, -7.0205e-01, -3.7986e-01, -5.7893e-01,\n",
      "         3.0747e-01, -2.1243e-02,  3.5246e-01, -2.5119e-01, -9.3597e-04,\n",
      "        -3.0555e-01, -1.0011e+00,  6.5736e-01, -1.9256e-02,  5.2677e-01,\n",
      "        -1.0669e-01, -3.7793e-01, -5.3913e-01,  1.4208e-01,  4.4693e-01,\n",
      "        -3.1716e-01,  3.0779e-01, -9.3854e-02, -6.7916e-01,  2.7398e-01,\n",
      "        -1.5625e-01,  2.8922e-01, -1.1230e-01, -1.1010e+00, -7.9936e-02,\n",
      "        -4.0612e-01,  2.0849e-02,  3.4710e-01,  4.3526e-02,  4.1308e-01,\n",
      "        -2.4231e-01, -9.4655e-01,  5.9509e-01,  4.2503e-01,  2.0579e-01,\n",
      "        -2.1274e-01,  3.5350e-02,  2.6313e-01, -2.9305e-01, -4.8487e-01,\n",
      "        -1.3051e-01, -6.9740e-01,  2.1987e-04,  7.6510e-01, -1.5280e-01,\n",
      "        -3.3351e-01,  3.0998e-01, -1.2360e-01,  3.9550e-01, -2.5940e-01,\n",
      "         6.4828e-02,  2.7397e-01, -1.5634e-01, -1.5517e-03,  6.4023e-01,\n",
      "         4.5707e-01, -7.3576e-01,  3.0117e-01, -4.9941e-01, -7.1724e-02,\n",
      "         4.7881e-01,  3.9970e-02,  6.5618e-01,  4.1189e-01,  4.7485e-01,\n",
      "        -6.1536e-01,  9.2372e-01,  1.2427e-01, -6.0887e-01, -3.6679e-01,\n",
      "        -6.6266e-01, -4.1378e-01,  8.5644e-02, -3.5291e-01,  9.0650e-01,\n",
      "        -2.6774e-01,  9.3557e-01,  2.5814e-01,  5.2646e-01,  1.5271e-01,\n",
      "         1.0880e-01, -9.9208e-02, -1.6872e-02,  3.1703e-02, -5.9293e-01,\n",
      "         3.7245e-02, -9.2498e-01, -5.0368e-01, -5.5661e-01, -5.3738e-01,\n",
      "        -5.1410e-01,  2.1918e-01,  5.5329e-01,  2.3826e-01,  6.1929e-02,\n",
      "         3.1898e-01,  2.5531e-01,  2.5802e-01,  5.3434e-01,  3.1996e-01,\n",
      "         1.0069e-01, -5.5496e-01, -3.7057e-01, -3.0339e-01, -1.5533e+00,\n",
      "        -5.9777e-01,  1.9452e-01, -3.6390e-01, -1.8436e-01,  5.1853e-01,\n",
      "        -2.0573e-01,  1.6028e-01, -4.1792e-01, -5.1243e-01, -1.3041e-01,\n",
      "         6.7688e-01,  5.5768e-01, -5.3552e-01, -3.6445e-01,  3.2987e-01,\n",
      "        -2.2302e-01,  1.1296e-01,  3.6002e-01, -2.2909e-01, -1.2944e-01,\n",
      "        -6.5539e-01,  4.0743e-01, -7.1309e-01, -2.8129e-01,  2.4215e-01,\n",
      "        -4.6964e-01, -2.4975e-02,  4.9898e-01, -8.8003e-01, -5.2670e-01,\n",
      "         1.7427e-01, -4.2441e-01,  7.8662e-01, -5.3916e-01,  3.1065e-01,\n",
      "         2.8586e-01, -3.8322e-01,  9.9570e-01, -1.5867e-01,  3.1806e-01,\n",
      "         5.9172e-01,  5.0590e-01, -1.8767e-01,  1.1740e-01,  6.0312e-01,\n",
      "         5.3131e-01, -4.3883e-01,  8.9231e-01,  1.0978e-01,  2.9768e-01,\n",
      "        -1.7925e-01, -3.8839e-01, -1.0702e-01, -1.8214e-01, -5.4149e-02,\n",
      "         3.7220e-01, -8.1579e-01, -5.9575e-01, -8.2093e-01, -4.2120e-01,\n",
      "         7.8664e-01, -1.8655e-01, -1.3514e-01,  3.9489e-01,  2.2791e-01,\n",
      "        -4.9966e-01, -2.2099e-01, -9.7539e-01,  4.4585e-01, -5.9411e-01,\n",
      "        -1.5309e+00,  6.6478e-01,  3.2396e-02, -3.4876e-01, -6.6410e-01,\n",
      "         1.0171e-01, -5.4313e-02,  1.1208e-01, -1.3051e+00,  2.9222e-01,\n",
      "         2.2818e-02, -3.7351e-01,  3.6517e-01,  5.8418e-01,  5.4525e-01,\n",
      "         1.4771e-01, -6.7846e-01,  4.3451e-01,  8.1183e-01,  9.4164e-01,\n",
      "        -1.6587e-01,  2.9264e-01,  1.0431e-01, -9.7528e-01,  8.0652e-01,\n",
      "        -5.1974e-01,  6.7096e-01, -6.9639e-02,  3.8821e-01, -1.3328e-01,\n",
      "        -5.3379e-01, -2.1545e-01, -3.0922e-02,  5.2758e-01, -6.4226e-01,\n",
      "         7.4214e-01, -9.9084e-02, -2.2626e-01, -8.4757e-01, -1.4464e-01,\n",
      "        -4.9659e-01,  9.6384e-01, -3.6248e-01, -4.5590e-01,  1.7444e-01,\n",
      "        -8.6812e-01, -3.6270e-01, -8.3892e-02, -6.0713e-01, -8.8571e-01,\n",
      "        -5.2017e-01, -1.0796e-01, -1.1842e-01,  7.4887e-01, -3.6315e-01,\n",
      "        -3.6595e-01, -6.3173e-01, -2.1045e-02,  5.7322e-01, -5.0686e-01,\n",
      "         3.6577e-01, -5.8337e-01,  2.4674e-01,  5.8519e-01,  5.8704e-01,\n",
      "        -1.0922e-01, -2.2033e-01, -1.0274e-01, -2.9973e-01,  1.1981e-01,\n",
      "         1.4441e-02,  1.7719e-01, -1.6776e-02, -5.2703e-01,  6.3709e-03,\n",
      "         8.5908e-01, -9.0771e-02,  8.8936e-02, -5.2244e-01,  1.6886e-01,\n",
      "        -6.9572e-02, -3.6009e-01, -4.3534e-01,  7.3869e-01,  1.0144e-02,\n",
      "         4.2082e-02,  3.1878e-01,  4.9873e-01, -3.8344e-01, -5.5373e-01,\n",
      "        -3.9139e-01, -5.0552e-01, -8.1062e-01, -2.1466e-01,  4.6705e-01,\n",
      "         8.1062e-02, -2.4331e-02,  2.2417e-01,  1.7968e-01, -6.6669e-01,\n",
      "         8.8549e-03, -1.0320e+00, -8.4253e-01,  8.3238e-02,  2.0019e-01,\n",
      "         8.5450e-01, -4.4028e-01,  1.2960e-01,  9.1979e-01,  5.6937e-01,\n",
      "         5.6918e-01,  2.3873e-01, -9.7526e-03, -2.6828e-01, -4.3488e-01,\n",
      "        -3.1654e-01, -6.7525e-01,  8.9604e-02, -3.8492e-01,  1.8651e-01,\n",
      "         4.2964e-01,  1.6951e-01, -2.1721e-01,  4.9334e-01, -6.4151e-01,\n",
      "        -1.9921e-01,  3.0178e-01,  6.5254e-01,  1.3441e-01, -4.5364e-01,\n",
      "        -4.3964e-01,  9.3067e-01,  4.9212e-02,  7.9005e-01, -2.7947e-01,\n",
      "        -1.2861e-01, -1.1191e-01, -9.7570e-03,  9.0923e-01,  1.2004e+00,\n",
      "        -2.4724e-01, -4.1714e-01, -7.1684e-01,  1.0332e-01, -1.7323e-01,\n",
      "        -3.5574e-01, -8.4381e-01,  1.1730e+00,  5.6873e-01,  2.5399e-01,\n",
      "        -4.8529e-01, -5.6345e-01,  5.7964e-01,  5.7392e-01, -7.4179e-02,\n",
      "         1.5523e-01,  1.7210e-01, -3.5008e-01,  1.6301e-01, -8.3165e-01,\n",
      "        -9.8661e-01, -6.0110e-01, -1.2554e-01, -1.7448e+00, -3.2454e-01,\n",
      "         3.0874e-01, -3.3978e-01,  2.9526e-02, -1.3952e-01, -2.8140e-01,\n",
      "        -4.4984e-01, -3.4724e-01, -1.0021e+00,  4.8977e-01,  2.6022e-02,\n",
      "        -1.0131e+00, -4.7002e-01, -8.9718e-02, -1.0494e+00, -1.0178e-01,\n",
      "         3.8123e-02,  1.4666e-01, -2.0754e-01,  9.7096e-02,  4.6700e-01,\n",
      "        -5.9059e-02,  6.0505e-02,  5.6263e-02,  6.5830e-01, -6.8960e-01,\n",
      "        -3.5968e-01,  6.2219e-01,  2.6003e-01,  6.1821e-01,  2.0742e-01,\n",
      "         3.5915e-02, -9.6990e-01,  3.3507e-01, -8.0716e-01, -4.2299e-01,\n",
      "        -4.4527e-01, -1.5236e-01,  2.7860e-01, -1.5025e-02, -1.8272e-01,\n",
      "         6.1361e-01,  2.7269e-01,  5.0596e-01,  8.9930e-01,  4.3492e-01,\n",
      "         7.4886e-01, -3.3694e-02, -4.8587e-01,  1.9097e-02,  9.8969e-02,\n",
      "        -7.8336e-02,  2.5597e-01,  5.0859e-01,  5.3146e-01, -5.5359e-01,\n",
      "         1.4253e-01, -2.6811e-01, -4.0937e-01, -5.3444e-02,  5.1993e-01,\n",
      "         6.1826e-01, -7.5056e-01, -6.7039e-01, -6.2210e-01, -7.4448e-01,\n",
      "         2.0625e-01, -2.5697e-01,  2.4676e-01, -3.3508e-01,  7.6297e-01,\n",
      "         3.1534e-01,  5.6650e-01,  6.5713e-01, -7.9394e-02,  7.7678e-01,\n",
      "         6.8464e-01,  1.8105e-01, -5.7682e-01,  2.3148e-01,  1.8354e-01,\n",
      "         5.3264e-01,  1.0745e-01,  5.9151e-01,  1.6822e-01,  3.3482e-01,\n",
      "         2.4336e-01,  3.8000e-01,  2.3643e-01,  8.1279e-02, -1.6266e-01,\n",
      "        -2.0972e-01,  3.6127e-01,  1.7537e-01, -2.2607e-02, -3.1598e-01,\n",
      "        -3.0899e-01,  2.6050e-01,  1.6448e-01, -8.3392e-01,  2.4559e-01,\n",
      "        -1.7996e-02, -5.8636e-02,  5.7999e-01,  2.5526e-02,  1.6725e-01,\n",
      "        -2.6779e-01,  1.6890e-01,  1.3000e+00,  7.3653e-01,  2.6302e-01,\n",
      "        -2.7671e-01, -2.4274e-01, -2.1897e-01,  1.6211e-01, -3.1851e-01,\n",
      "         3.9199e-02,  1.4024e-01, -4.0862e-01,  1.9422e-01, -9.6029e-02,\n",
      "        -3.0628e-01, -3.8949e-01, -1.8944e-01, -9.0315e-01, -8.7265e-01,\n",
      "        -4.3267e-01, -7.3842e-02, -8.3184e-01, -5.2276e-01,  1.1610e-01],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "  output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Default Model From concept bottle neck model from x to c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "112"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCBMxtoc = ModelXtoC(pretrained = True, freeze = True, num_classes = 200, use_aux = True, n_attributes = 112, expand_dim = 0, three_class = False)\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    modelCBMxtoc.to('cuda')\n",
    "modelCBMxtoc.eval()\n",
    "len(modelCBMxtoc(input_batch))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## self designed Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "from CUB.template_model import BasicConv2d, InceptionA, InceptionAux, InceptionD, InceptionE, InceptionC, InceptionB, FC\n",
    "from torch.nn import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class InceptionF(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionF, self).__init__()\n",
    "        self.convo = BasicConv2d(in_channels, 320, kernel_size=(3, 3))\n",
    "    def forward(self, x):\n",
    "        return self.convo(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_urls = {\n",
    "    # Downloaded inception model (optional)\n",
    "    'downloaded': 'pretrained/inception_v3_google-1a9a5a14.pth',\n",
    "    # Inception v3 ported from TensorFlow\n",
    "    'inception_v3_google': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n",
    "}\n",
    "\n",
    "def inception_v3(pretrained, freeze, **kwargs):\n",
    "    \"\"\"Inception v3 model architecture from\n",
    "    `\"Rethinking the Inception Architecture for Computer Vision\" <http://arxiv.org/abs/1512.00567>`_.\n",
    "\n",
    "    .. note::\n",
    "        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n",
    "        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        transform_input (bool): If True, preprocesses the input according to the method with which it\n",
    "            was trained on ImageNet. Default: *False*\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        if 'transform_input' not in kwargs:\n",
    "            kwargs['transform_input'] = True\n",
    "        model = Inception3(**kwargs)\n",
    "        if os.path.exists(model_urls.get('downloaded')):\n",
    "            model.load_partial_state_dict(torch.load(model_urls['downloaded']))\n",
    "        else:\n",
    "            model.load_partial_state_dict(model_zoo.load_url(model_urls['inception_v3_google']))\n",
    "        if freeze:  # only finetune fc layer\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'fc' not in name:  # and 'Mixed_7c' not in name:\n",
    "                    param.requires_grad = False\n",
    "        return model\n",
    "\n",
    "    return Inception3(**kwargs)\n",
    "\n",
    "class Inception3(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, aux_logits=True, transform_input=False, n_attributes=0, bottleneck=False, expand_dim=0, three_class=False, connect_CY=False, conceptFilter = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        num_classes: number of main task classes\n",
    "        aux_logits: whether to also output auxiliary logits\n",
    "        transform input: whether to invert the transformation by ImageNet (should be set to True later on)\n",
    "        n_attributes: number of attributes to predict\n",
    "        bottleneck: whether to make X -> A model\n",
    "        expand_dim: if not 0, add an additional fc layer with expand_dim neurons\n",
    "        three_class: whether to count not visible as a separate class for predicting attribute\n",
    "        \"\"\"\n",
    "        super(Inception3, self).__init__()\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "        self.n_attributes = n_attributes\n",
    "        self.bottleneck = bottleneck\n",
    "        self.conceptFilter = conceptFilter\n",
    "        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, stride=2)\n",
    "        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)\n",
    "        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n",
    "        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n",
    "        self.Mixed_5b = InceptionA(192, pool_features=32)\n",
    "        self.Mixed_5c = InceptionA(256, pool_features=64)\n",
    "        self.Mixed_5d = InceptionA(288, pool_features=64)\n",
    "        self.Mixed_6a = InceptionB(288)\n",
    "        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n",
    "        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n",
    "        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n",
    "        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n",
    "        if aux_logits:\n",
    "            self.AuxLogits = InceptionAux(768, num_classes, n_attributes=self.n_attributes, bottleneck=bottleneck, \\\n",
    "                                                expand_dim=expand_dim, three_class=three_class, connect_CY=connect_CY)\n",
    "        self.Mixed_7a = InceptionD(768)\n",
    "        self.Mixed_7b = InceptionE(1280)\n",
    "        self.Mixed_7c = InceptionE(2048)\n",
    "\n",
    "        #used to filter the concepts out\n",
    "        self.ConceptFilter =  torch.nn.ModuleList([InceptionF(2048) for i in range(n_attributes)])\n",
    "        self.all_fc = nn.ModuleList() #separate fc layer for each prediction task. If main task is involved, it's always the first fc in the list\n",
    "        self.all_fc2 = nn.ModuleList() #separate fc layer for each prediction task. If main task is involved, it's always the first fc in the list\n",
    "\n",
    "        if connect_CY:\n",
    "            self.cy_fc = FC(n_attributes, num_classes, expand_dim)\n",
    "        else:\n",
    "            self.cy_fc = None\n",
    "\n",
    "        if self.n_attributes > 0:\n",
    "            if not bottleneck: #multitasking\n",
    "                self.all_fc.append(FC(2048, num_classes, expand_dim))\n",
    "            for i in range(self.n_attributes):\n",
    "                self.all_fc.append(FC(2048, 1, expand_dim))\n",
    "        else:\n",
    "            self.all_fc.append(FC(2048, num_classes, expand_dim))\n",
    "\n",
    "        if self.n_attributes > 0:\n",
    "            if not bottleneck: #multitasking\n",
    "                self.all_fc2.append(FC(320, num_classes, expand_dim))\n",
    "            for i in range(self.n_attributes):\n",
    "                self.all_fc2.append(FC(320, 1, expand_dim))\n",
    "        else:\n",
    "            self.all_fc2.append(FC(320, num_classes, expand_dim))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n",
    "                X = stats.truncnorm(-2, 2, scale=stddev)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "        # N x 3 x 299 x 299\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # N x 32 x 149 x 149\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # N x 32 x 147 x 147\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # N x 64 x 147 x 147\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # N x 64 x 73 x 73\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # N x 80 x 73 x 73\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # N x 192 x 71 x 71\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # N x 192 x 35 x 35\n",
    "        x = self.Mixed_5b(x)\n",
    "        # N x 256 x 35 x 35\n",
    "        x = self.Mixed_5c(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_5d(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_6a(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6b(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6c(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6d(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6e(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        if self.training and self.aux_logits:\n",
    "            out_aux = self.AuxLogits(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_7a(x)\n",
    "        # N x 1280 x 8 x 8\n",
    "        x = self.Mixed_7b(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        x = self.Mixed_7c(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        # Adaptive average pooling\n",
    "        if self.conceptFilter:\n",
    "            filterOut = self._concept_filters(x)\n",
    "            # [N x 2048 x 8 x 8] list of size of the attributes\n",
    "            filterOut = [F.adaptive_avg_pool2d(x, (1, 1)) for x in filterOut]\n",
    "            filterOut = [F.dropout(x, training=self.training) for x in filterOut]\n",
    "            filterOut = [x.view(x.size(0), -1) for x in filterOut]\n",
    "            filterOut = [fc(x) for fc,x in zip(self.all_fc2, filterOut)]\n",
    "            return filterOut\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # N x 2048\n",
    "        out = []\n",
    "        for fc in self.all_fc:\n",
    "            out.append(fc(x))\n",
    "        if self.n_attributes > 0 and not self.bottleneck and self.cy_fc is not None:\n",
    "            attr_preds = torch.cat(out[1:], dim=1)\n",
    "            out[0] += self.cy_fc(attr_preds)\n",
    "        if self.training and self.aux_logits:\n",
    "            return out, out_aux\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def _concept_filters(self, x):\n",
    "        return [self.ConceptFilter[i](x) for i in range(self.n_attributes)]\n",
    "\n",
    "    def load_partial_state_dict(self, state_dict):\n",
    "        \"\"\"\n",
    "        If dimensions of the current model doesn't match the pretrained one (esp for fc layer), load whichever weights that match\n",
    "        \"\"\"\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if name not in own_state or 'fc' in name:\n",
    "                continue\n",
    "            if isinstance(param, Parameter):\n",
    "                param = param.data\n",
    "            own_state[name].copy_(param)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "modelNew = inception_v3(pretrained = True, freeze = True, num_classes = 200,  n_attributes = 112, expand_dim = 0, three_class = False, bottleneck=True, conceptFilter = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "112"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    modelNew.to('cuda')\n",
    "modelNew.eval()\n",
    "len(modelNew(input_batch))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[-34.3326]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.5820]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-8.7992]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-10.0736]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-22.8786]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[26.2267]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-11.8767]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[3.2959]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-20.7309]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-33.6099]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[4.0887]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-40.7761]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-24.7010]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.1098]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-11.4188]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[11.6811]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.2582]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.0909]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-47.2927]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-24.0706]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[5.2081]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-33.6448]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[8.1213]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[4.3411]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-34.8341]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[7.2401]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-6.0959]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[26.0653]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[40.9040]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[33.0287]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.8174]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[4.8834]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-11.7895]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[20.0322]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-19.2001]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.4066]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-6.6293]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.4703]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[25.8166]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-63.7849]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-20.7246]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-29.5342]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-20.0531]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-20.7355]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[15.8589]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[17.2271]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.8410]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[4.3500]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-19.2550]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-17.8823]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[35.7384]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[6.0395]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-13.6055]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-31.3029]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.7129]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-17.3378]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[21.7129]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-7.7537]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[10.3627]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-3.0430]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[24.0728]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-6.4917]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-7.2419]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.9207]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[10.0416]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[9.5474]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[36.7214]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-44.0556]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[17.8971]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-25.3157]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-37.3281]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[13.3241]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[21.2965]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.7863]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-23.8780]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[28.0419]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.1603]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[38.3622]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-28.6924]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-14.2981]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-14.4853]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[10.7499]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-16.8439]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.7272]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[8.4680]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-18.5911]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[25.6773]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-30.6346]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-7.4511]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-38.5832]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-18.2439]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-5.2016]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[31.1255]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[26.9085]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-50.7118]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[12.2542]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[10.7286]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[10.5951]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-21.6888]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-12.7484]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-16.5840]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-47.5722]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[10.3111]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-37.9731]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[7.8564]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[17.4809]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[18.2269]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-9.3764]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-14.9705]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.3101]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.6615]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[8.0684]], device='cuda:0', grad_fn=<AddmmBackward0>)]"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelNew(input_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[-0.9830]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.4407]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.4971]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.2671]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.0709]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.9529]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-2.9429]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.0055]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.8997]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.6917]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.3093]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.2598]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.9011]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.0315]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[3.5385]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.7489]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.3602]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.1901]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.7388]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.0995]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.5722]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.5956]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.3669]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.3092]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.4929]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.3152]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.5771]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.0566]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.7604]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.1107]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.8692]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.1990]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.6961]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.0175]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.3201]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.2477]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.1138]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.0224]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.5447]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[4.1968]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.7231]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.7284]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.8576]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.9807]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.0523]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.2647]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.9107]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.0508]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.6875]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.9136]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.8747]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.2836]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.0826]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.2761]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-2.3243]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.5891]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.0876]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.3677]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.9955]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.2108]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.4769]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.7747]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.6198]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-2.7948]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.7475]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.3599]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.1988]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-2.4127]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.0752]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.8171]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.2006]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.3319]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.3369]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.4439]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.6042]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.8468]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.8956]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.0166]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.0246]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.7135]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.8787]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.1929]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.5952]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.3441]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.0655]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-2.5810]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.1138]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.0477]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.1101]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.2358]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.3864]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.5488]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.5739]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.8707]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.8875]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.8437]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.2202]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.7271]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-2.0351]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.1096]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.1822]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.7328]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[2.5102]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-0.2366]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.1065]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.2943]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.8720]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[-1.3102]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.3348]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.4400]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[0.4854]], device='cuda:0', grad_fn=<AddmmBackward0>),\n tensor([[1.9562]], device='cuda:0', grad_fn=<AddmmBackward0>)]"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCBMxtoc(input_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}